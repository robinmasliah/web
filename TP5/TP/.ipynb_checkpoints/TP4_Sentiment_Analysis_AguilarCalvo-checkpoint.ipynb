{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP5 : Sentiment Analysis\n",
    "## AGUILAR CALVO Heber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv as csv\n",
    "import nltk as nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sentiwordnet import SentiWordNetCorpusReader, SentiSynset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitements\n",
    "\n",
    "Les tweets contiennent des caractères spéciaux susceptibles de nuire à la mise en place des méthodes\n",
    "d’analyse d’opinions. Ecrire un programme permettant pour chaque tweet de :\n",
    "* récupérer le texte associé\n",
    "* segmenter en tokens\n",
    "* supprimer les urls\n",
    "* nettoyer les caractères inhérents à la structure d’un tweet\n",
    "* corriger les abréviations et les spécificités langagières des tweets à l’aide du diction-\n",
    "naire DicoSlang disponible ici : http://perso.telecom-paristech.fr/~clavel/DonneesTweets/SlangLookupTable.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_url(string):\n",
    "    '''\n",
    "    Input : string\n",
    "    --------------\n",
    "    Output : string\n",
    "    '''\n",
    "    \n",
    "    url = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', string) # Identify urls\n",
    "    string_without_url = string.replace(str(url), '') # Get rid of urls\n",
    "    \n",
    "    return string_without_url\n",
    "\n",
    "def tokenize(twit):\n",
    "    '''\n",
    "    Input : twit (string)\n",
    "    --------------\n",
    "    Output : list of tokens \n",
    "    '''\n",
    "    \n",
    "    tokens = nltk.word_tokenize(str(twit))\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "\n",
    "def clean_twitter_inherent_chars(tokens_list):\n",
    "    '''\n",
    "    Input : list of tokens\n",
    "    --------------\n",
    "    Output : clean list of tokens\n",
    "    '''\n",
    "    \n",
    "    clean_liste = [token for token in tokens_list if token!=\"@\" and token!=\"#\"] \n",
    "    \n",
    "    return clean_liste\n",
    "\n",
    "\n",
    "def slang_dictionnary():\n",
    "    slang_dict = {}\n",
    "    with open('Lexiques/SlangLookupTable.txt', encoding=\"ISO-8859-1\") as slangtxt:\n",
    "        for line in slangtxt:\n",
    "            (key, val) = line.split('\\t')\n",
    "            slang_dict[key] = val\n",
    "    return slang_dict\n",
    "\n",
    "\n",
    "def get_rid_slang(clean_tokens, slangDict):\n",
    "    '''\n",
    "    Input: clean tokes (list)\n",
    "    -------------------------\n",
    "    Output: preprocessed twit (without slang) \n",
    "    '''\n",
    "    preprocessed_twit = [slangDict[token] if token in slangDict.keys() else token for token in clean_tokens]\n",
    "    \n",
    "    return preprocessed_twit\n",
    "\n",
    "def pretraitement(text, slangDict):\n",
    "    '''\n",
    "    Input: list (list de strings)\n",
    "    ------------------\n",
    "    Output: prepprocessed twit\n",
    "    '''\n",
    "    twit_txt = text[5]                                          # 1. Get twit from list of twits\n",
    "    twit_no_url = remove_url(twit_txt)                          # 2. Remove url\n",
    "    #twit_no_url = re.sub(r'[\\W_]+', ' ', twit_no_url)           # Extra: remove puctuation symbols               \n",
    "    tokens = tokenize(twit_no_url)                              # 3. Token segmentation\n",
    "    clean_tokens = clean_twitter_inherent_chars(tokens)         # 4. Clean inherent twit characters\n",
    "    preprocessed_twit = get_rid_slang(clean_tokens, slangDict)  # 5. Correct slang specifications\n",
    "\n",
    "    return preprocessed_twit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etiquetage grammatical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def etiquetage_grammatical(preprocessed_twit):\n",
    "    '''\n",
    "    Input: preprocessed_twit (a string of words)\n",
    "    ---------------------------\n",
    "    Output: tagged twit \n",
    "    '''\n",
    "\n",
    "    tagged_twit = nltk.pos_tag(preprocessed_twit)\n",
    "    \n",
    "    return tagged_twit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme de détection v1 : appel au dictionnaire Sentiwordnet :\n",
    "\n",
    "Pour cette étape, vous devez développer un programme permettant :\n",
    "* de récupérer uniquement les mots correspondant à des adjectifs, noms, adverbes et verbes\n",
    "* d’accéder aux scores (positifs et négatifs) des synsets dans la librairie NLTK. Ce script définira dans une classe Python l’objet SentiSynset sur le même modèle que le Synset développé dans NLTK pour WordNet, et permettra de lire le tableau de SentiWordNet comme suit.\n",
    "* de calculer pour chaque mot les scores associés à leur premier synset,\n",
    "* de calculer pour chaque tweet la somme des scores positifs et négatifs des SentiSynsets du tweet,\n",
    "* de comparez la somme des scores positifs et des scores négatifs de chaque tweet pour décider de la classe à associer au tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#raw_twits = {}\n",
    "dataFile = open(\"testdata.manual.2009.06.14.csv\", 'rt')\n",
    "dataReader = csv.reader(dataFile,delimiter=',')\n",
    "for text in dataReader:\n",
    "    preprocessed_twit = pretraitement(text, slangDict=slang_dictionnary())\n",
    "    taggedData = etiquetage_grammatical(preprocessed_twit)\n",
    "    #print(taggedData)\n",
    "#dataFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.625, 1.375, 'Negative')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_adj_adv_verb_noun(tag):\n",
    "    '''\n",
    "    Tags :\n",
    "        JJ : adjective or numeral; JJR : adjective, comparative; JJS: adjective, superlative\n",
    "        NN: noun, singular; NNP: noun, proper, singular; NNPS: noun, proper, plural; NNS: noun, plural \n",
    "        RB: adverb; RBR: adverb, comparative; RBS: adverb, superlativeightest worst\n",
    "        VB: verb, base form; VBD: verb, past tense; VBG: verb, present participle or gerund\n",
    "        VBN: verb, past participle; VBP: verb, present tense, not 3rd person singular\n",
    "        VBZ: verb, present tense, 3rd person singular bases \n",
    "    '''\n",
    "    adjectif_tags = ['JJ','JJR','JJS']\n",
    "    noun_tags = ['NN','NNP','NNPS','NNS']\n",
    "    adverb_tags = ['RB','RBR','RBS']\n",
    "    verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    \n",
    "    if tag in adjectif_tags or tag in noun_tags or tag in adverb_tags or tag in verb_tags:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def filter_adj_adv_verb_noun(twit):\n",
    "    filtered_pos_words = list(filter(lambda word: is_adj_adv_verb_noun(word[1])==True, twit))\n",
    "    return list(map(lambda word: word[0],filtered_pos_words)) \n",
    "\n",
    "    \n",
    "def first_synset(word):\n",
    "    list_synsets = wn.synsets(word)\n",
    "    if len(list_synsets)>0:\n",
    "        return list_synsets[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_first_synsets(filtered_twit):\n",
    "    list_synsets = list(map(lambda word : first_synset(word),filtered_twit))\n",
    "    return list(map(lambda word: word.name(),filter(lambda synset: synset is not None ,list_synsets)))\n",
    "\n",
    "def get_pos_scores(list_synsets):\n",
    "    swn_filename = 'Lexiques/SentiWordNet_3.0.0_20130122.txt'\n",
    "    swn = SentiWordNetCorpusReader(\"\",swn_filename)\n",
    "    return list(map(lambda synset: swn.senti_synset(synset).pos_score(),list_synsets))    \n",
    "\n",
    "def get_neg_scores(list_synsets):\n",
    "    swn_filename = 'Lexiques/SentiWordNet_3.0.0_20130122.txt'\n",
    "    swn = SentiWordNetCorpusReader(\"\",swn_filename)\n",
    "    return list(map(lambda synset: swn.senti_synset(synset).neg_score(),list_synsets))    \n",
    "\n",
    "\n",
    "def sum_pos_neg_scores(list_pos_scores, list_neg_scores):\n",
    "    '''\n",
    "    Input:\n",
    "        List of positive scores (in the first place) \n",
    "        List of negative scores\n",
    "    '''\n",
    "    sum_pos = sum(list_pos_scores)\n",
    "    sum_neg = sum(list_neg_scores)\n",
    "    return sum_pos, sum_neg\n",
    "\n",
    "def get_sentiment(sum_pos_scores, sum_neg_scores):\n",
    "    '''\n",
    "    Input:\n",
    "        Sum of positive scores (in first position) \n",
    "        Sum of negative scores\n",
    "    '''\n",
    "    if sum_pos_scores < sum_neg_scores:\n",
    "        return 'Negative'\n",
    "    elif sum_pos_scores == sum_neg_scores:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'    \n",
    "    \n",
    "    \n",
    "def get_scores_and_sentiment_from_twit(tagged_twit):\n",
    "    '''\n",
    "    Input:\n",
    "        Tagged and preprocessed twit\n",
    "    Output:\n",
    "        Positive score, Negative score, Sentiment\n",
    "    '''\n",
    "    \n",
    "    filtered_words = filter_adj_adv_verb_noun(tagged_twit)\n",
    "    list_synsets = get_first_synsets(filtered_words)\n",
    "    list_pos_scores = get_pos_scores(list_synsets)\n",
    "    list_neg_scores = get_neg_scores(list_synsets)\n",
    "    pos_score, neg_score = sum_pos_neg_scores(list_pos_scores, list_neg_scores)\n",
    "    sentiment = get_sentiment(pos_score, neg_score)\n",
    "    \n",
    "    return pos_score, neg_score, sentiment\n",
    "\n",
    "get_scores_and_sentiment_from_twit(taggedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme de détection v2 : gestion de la négation et des modifieurs\n",
    "* multiplie par 2 le score négatif et le score positif associés au mot si le mot précédent est un modifieur ;\n",
    "* utilise uniquement le score négatif du mot pour le score positif global du tweet et le score positif du mot pour le score négatif global du tweet si le mot précédent est une négation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "['Reading', 'VBG'] is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-91d3cb8d845a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#    words_from_twit = map(lambda word: word[0], tagged_wit)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtaggedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Reading'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VBG'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: ['Reading', 'VBG'] is not in list"
     ]
    }
   ],
   "source": [
    "def NegationReader():\n",
    "    negatingWordList = []\n",
    "    with open('Lexiques/NegatingWordList.txt', 'rt') as negationtxt:\n",
    "        for line in negationtxt:\n",
    "            negation, tab = line.split('\\n') \n",
    "            negatingWordList.append(negation)\n",
    "    return negatingWordList\n",
    "\n",
    "def ModifierReader():\n",
    "    modifierWordList = []\n",
    "    with open('Lexiques/BoosterWordList.txt', 'rt') as modifiertxt:\n",
    "        for line in modifiertxt:\n",
    "            modifier, tab = line.split('\\t') \n",
    "            #print(modifier)\n",
    "            modifierWordList.append(modifier)\n",
    "    return modifierWordList\n",
    "\n",
    "mWL = ModifierReader()\n",
    "nWL = NegationReader()\n",
    "\n",
    "#---------------------------------\n",
    "\n",
    "#def find_precedent_word(tagged_twit, word):\n",
    "#    words_from_twit = map(lambda word: word[0], tagged_wit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme de détection v3 : gestion des emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ma version v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
